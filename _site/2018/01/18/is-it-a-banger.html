<!DOCTYPE html>
<html>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
     inlineMath: [ ['$','$'], ["\\(","\\)"] ],
     processEscapes: true
   }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Favicon -->
  <link rel="icon" href="//favicon.png">

  <!-- Meta information -->
  <title>Is it a banger? Audio classification in tensorflow</title>
  <meta name="description"
        content="">
  <link rel="canonical"
        href="https://ddervs.github.io///2018/01/18/is-it-a-banger.html" />

  <!-- hack.css -->
  <link rel="stylesheet" href="https://npmcdn.com/hack/dist/hack.css" />
  

  <!-- Prism.js -->
  <link rel="stylesheet" href="https://npmcdn.com/prismjs@1.5.1/themes/prism.css" />

  <!-- Custom style -->
  <link rel="stylesheet" href="//css/main.css" />

  <!-- Feed -->
  <link rel="alternate" type="application/rss+xml" title="Danial Dervovic"
        href="//feed.xml" />

  <!-- 'jekyll-seo' plugin -->
  <!-- Begin Jekyll SEO tag v2.0.0 -->
<title>Is it a banger? Audio classification in tensorflow - Danial Dervovic</title>
<meta property="og:title" content="Is it a banger? Audio classification in tensorflow" />
<meta name="description" content="" />
<meta property="og:description" content="" />
<link rel="canonical" href="https://ddervs.github.io///2018/01/18/is-it-a-banger.html" />
<meta property="og:url" content="https://ddervs.github.io///2018/01/18/is-it-a-banger.html" />
<meta property="og:site_name" content="Danial Dervovic" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-01-18T12:14:01+00:00" />
<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "Is it a banger? Audio classification in tensorflow",
    "datePublished": "2018-01-18T12:14:01+00:00",
    "description": "",
    "url": "https://ddervs.github.io///2018/01/18/is-it-a-banger.html"
  }
</script>
<!-- End Jekyll SEO tag -->
</head>



  
  <body class="hack">
  

    <a href="https://github.com/ddervs/" class="github-corner">


  <svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg>


</a>

<style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


    <div class="container">
      <div class="grid">
        <aside class="cell -3of12" role="navigation">
          <div class="t-hackcss-navigation">
  <h2 class="t-hackcss-navigation-heading">Menu</h2>

  <nav class="menu" role="menubar">
    
    
      <a class="menu-item "
        role="menuitem" href="//" title="">
        Home <div class="pull-right">»</div>
      </a>
    
      <a class="menu-item "
        role="menuitem" href="//blog" title="">
        Blog <div class="pull-right">»</div>
      </a>
    
      <a class="menu-item "
        role="menuitem" href="//papers" title="">
        Papers <div class="pull-right">»</div>
      </a>
    
      <a class="menu-item "
        role="menuitem" href="//outreach" title="">
        Outreach <div class="pull-right">»</div>
      </a>
    
      <a class="menu-item "
        role="menuitem" href="//notes" title="">
        Notes <div class="pull-right">»</div>
      </a>
    
  </nav>

</div>

        </aside>

        <main class="cell -9of12">
          <style>
.grid {
    flex-direction: column;
  }

.cell {
flex: 0 0 auto;
}
</style>
<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     tex2jax: {
     inlineMath: [ ['$','$'], ["\\(","\\)"] ],
     processEscapes: true
   }
});
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Is it a banger? Audio classification in tensorflow</h1>
    <p class="post-meta">
      <time datetime="2018-01-18T12:14:01+00:00"
            itemprop="datePublished"
            class="media-heading">
        Jan 18, 2018
      </time>

      
        <span itemprop="author" itemscope itemtype="http://schema.org/Person">
          <span itemprop="name">ddervs</span>
        </span>
      
    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <div align="middle"><img src="//assets/is_it_a_banger_files/Tom_Haverford.gif" /></div>

<p>In <em>Parks and Recreation</em> Season 6 Episode 18 “Prom”, Tom Haverford famously tells us about his test of whether a song is a “banger” or not. There are many questions in this test: “does it feature any acoutic instruments?”, “how many drops?”, “how dope are the drops?” etc.</p>

<p>I think we can make his test even more rigorous: why don’t we use a deep neural network, trained on examples of bangers (and non-bangers), to tell us if a song is banger or not?</p>

<p>In this jupyter notebook, we’re going to construct, train and test this neural network.</p>

<h2 id="initial-environment">Initial Environment</h2>

<div class="highlighter-rouge"><pre class="highlight"><code>import matplotlib.pyplot as plt
import librosa.display
import numpy as np
np.random.seed(1337)
import pandas as pd
%matplotlib inline
</code></pre>
</div>

<h2 id="the-dataset">The Dataset</h2>

<div class="highlighter-rouge"><pre class="highlight"><code>df = pd.read_pickle("../data/processed_dataset.pkl")
</code></pre>
</div>

<p>This data set was generated using the instructions in <a href="https://nbviewer.jupyter.org/github/ddervs/is_it_a_banger/blob/master/scripts/load_files.ipynb">this notebook</a>. Let’s take a look.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>df[:9]
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>audio</th>
      <th>label</th>
      <th>label_one_hot</th>
      <th>log_specgram</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Cliff Richard - Greatest Hits 1958-1962 (Not Now Music) [Full Album]_0415.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>not_a_banger</td>
      <td>[1.0, 0.0]</td>
      <td>[[-80.0, -54.1524, -35.3907, -33.0633, -39.626...</td>
    </tr>
    <tr>
      <th>Selected New Year Mix_0121.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>banger</td>
      <td>[0.0, 1.0]</td>
      <td>[[-67.3112, -51.5708, -53.4622, -72.6484, -80....</td>
    </tr>
    <tr>
      <th>Rihanna - Stay ft. Mikky Ekko_0036.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>not_a_banger</td>
      <td>[1.0, 0.0]</td>
      <td>[[-64.2413, -50.564, -57.0061, -37.2135, -37.0...</td>
    </tr>
    <tr>
      <th>The Lumineers - Slow It Down (Live on KEXP)_0049.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>not_a_banger</td>
      <td>[1.0, 0.0]</td>
      <td>[[-80.0, -73.9336, -59.1297, -49.4456, -45.314...</td>
    </tr>
    <tr>
      <th>Passenger _ Let Her Go (Official Video)_0016.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>not_a_banger</td>
      <td>[1.0, 0.0]</td>
      <td>[[-80.0, -79.4122, -63.2455, -56.2228, -56.834...</td>
    </tr>
    <tr>
      <th>Low Steppa - Vocal Loop (Premiere)_0032.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>banger</td>
      <td>[0.0, 1.0]</td>
      <td>[[-65.6515, -31.3697, -21.9142, -25.2813, -61....</td>
    </tr>
    <tr>
      <th>Stardust - Music Sounds Better (Mistrix Dub) (Free Download)_0049.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>banger</td>
      <td>[0.0, 1.0]</td>
      <td>[[-80.0, -80.0, -78.6725, -79.2538, -80.0, -80...</td>
    </tr>
    <tr>
      <th>Ed Sheeran - Thinking Out Loud [Official Video]_0033.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>not_a_banger</td>
      <td>[1.0, 0.0]</td>
      <td>[[-80.0, -65.3586, -53.2574, -44.407, -50.1324...</td>
    </tr>
    <tr>
      <th>Best Of 2017 Tech House Yearmix_0145.wav</th>
      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>
      <td>banger</td>
      <td>[0.0, 1.0]</td>
      <td>[[-80.0, -57.0543, -39.8118, -61.7071, -38.360...</td>
    </tr>
  </tbody>
</table>
</div>

<p>We can see in the first column the names of the tracks (in <code class="highlighter-rouge">.wav</code> format) with a numeric identifier at the end. Each track has been clipped into 5 second segments (at 22.05kHz sample rate) and the identifier tells us which segment we have.</p>

<p>The <code class="highlighter-rouge">audio</code> column is a numpy array with the audio sample values.</p>

<p>The <code class="highlighter-rouge">label</code> column tells us if the given file is labelled as a banger or not. For the most part, the labels are obvious to us (but not the machine): Ed Sheeran, The Lumineers, Cliff Richard… clearly NOT A BANGER. Various tech house mixes and artists - BANGERZ.</p>

<p>The <code class="highlighter-rouge">label_one_hot</code> column gives us the vectorised, “one-hot” encoding of the label. <code class="highlighter-rouge">[0.0, 1.0] == banger</code>, <code class="highlighter-rouge">[1.0, 0.0] == not_a_banger</code>.</p>

<p>The final column, <code class="highlighter-rouge">log_specgram</code>, is the most interesting and what will comprise our features input to the neural net. It comprises the <em>log spectrogram</em> of the audio signal. This is the absolute value squared <a href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform">Short Time Fourier Transform</a> of the audio signal. This gives us the frequency content of the signal within short time windows.</p>

<p>We’re going to use a common image classification tool, a ConvNet, on the log spectrogram image to do our classification.</p>

<p>Let’s take a closer look at the dataset.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>bangerz = df.loc[df['label'] == "banger"]
clangerz = df.loc[df['label'] == "not_a_banger"]
num_bangerz = bangerz.index.size
num_clangerz = clangerz.index.size

print("Dataset has %g audio clips." % df.index.size)
print( "This is split between %g \"banger\"s and %g \"not_a_banger\"s" % (num_bangerz, num_clangerz) )

Dataset has 875 audio clips.
This is split between 422 "banger"s and 453 "not_a_banger"s
</code></pre>
</div>

<p>So we are split more-or-less 50:50 between bangers and clangers. Now we want to look at the audio signal and log spectrogram for some examples.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>def plot_waveforms(df, idx):
    audio = df.iloc[idx].audio
    log_specgram = df.iloc[idx].log_specgram
    filename = df.iloc[idx].name
    label = df.iloc[idx].label
    # audio is np.array holding sample values, log_specgram is 2-dim np.array
    plt.figure(figsize=(15,6))
    plt.subplot(1, 2, 1)
    librosa.display.waveplot(audio, sr=22050)
    plt.subplot(1, 2, 2)
    librosa.display.specshow(log_specgram, x_axis='time',y_axis='log')
    plt.colorbar(format='%+2.0f dB')
    plt.suptitle(filename + ", label = \"" + label + "\".")



[plot_waveforms(bangerz, i) for i in [0, 1, 2]];
</code></pre>
</div>

<p><img src="//assets/is_it_a_banger_files/is_it_a_banger_13_0.png" alt="png" /></p>

<p><img src="//assets/is_it_a_banger_files/is_it_a_banger_13_1.png" alt="png" /></p>

<p><img src="//assets/is_it_a_banger_files/is_it_a_banger_13_2.png" alt="png" /></p>

<div style="padding-top:1cm;"></div>
<p>We can see for the first two bangers sharp, rhythmic, percussive signal, focused on the low end of the frequency spectrum. This is the kick drum!</p>

<p>In the third banger, we are likely in a section where the producer has used a high-pass filter, since there is virtually no low-frequency content here, yet we can still see some regularity from the kick in the higher end of the spectrum.</p>

<p>Now for the clangers!</p>
<div style="padding-bottom:1cm;"></div>

<div class="highlighter-rouge"><pre class="highlight"><code>[plot_waveforms(clangerz, i) for i in [0, 1, 2]];
</code></pre>
</div>

<p><img src="//assets/is_it_a_banger_files/is_it_a_banger_15_0.png" alt="png" /></p>

<p><img src="//assets/is_it_a_banger_files/is_it_a_banger_15_1.png" alt="png" /></p>

<p><img src="//assets/is_it_a_banger_files/is_it_a_banger_15_2.png" alt="png" /></p>

<div style="padding-top:1cm;"></div>
<p>Here we see a less percussive, rhythic signal across the board, with far less low-frequency content.</p>

<p>Hopefully our ConvNet will be able to use this to its advantage.</p>
<div style="padding-bottom:1cm;"></div>

<h3 id="establish-baseline">Establish baseline</h3>

<p>We can calculate a baseline classification accuracy, if we just choose the majority label in the dataset for any example. This is the accuracy we need to beat.</p>

<p>Ideally, we should run “Haverford’s algorithm” and compare, but I really didn’t feel like doing this for 875 examples! Volunteers welcome…</p>

<div class="highlighter-rouge"><pre class="highlight"><code>naive_accuracy = (max(num_bangerz, num_clangerz) / (float)(df.index.size))
print ("This is the accuracy if we always guess max{#banger, #not_a_banger}: %.3f" % naive_accuracy)

This is the accuracy if we always guess max{#banger, #not_a_banger}: 0.518
</code></pre>
</div>

<h3 id="form-the-training-and-testing-data-sets">Form the training and testing data sets¶</h3>

<p>Let’s set aside 80% of the data for training and 20% for testing.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>train_frac = 0.8

def split_train_test(df, train_frac=0.8):
    include = np.random.rand(*df.index.shape)
    is_train = include &lt; train_frac
    train_data = df[is_train]
    test_data = df[~is_train]
    return train_data, test_data
                
train_data, test_data = split_train_test(df, train_frac)


print( "Training data has %g clips, test data has %g clips." % (train_data.index.size, test_data.index.size))

Training data has 711 clips, test data has 164 clips.
</code></pre>
</div>

<h2 id="tensorflow">Tensorflow</h2>

<p>Having prepped the training and test datasets, we’re ready to set up our ConvNet. We will closely follow the structure of the tensorflow <a href="https://www.tensorflow.org/get_started/mnist/pros">deep MNIST</a> example neural net with some small modifications – if it ain’t broke, don’t fix it!</p>

<p>The deep network will look something like this:</p>

<div align="middle"><img src="//assets/is_it_a_banger_files/ConvNet.png" width="85%" /></div>

<p>We feed the image of the log spectrogram into a convolutional layer, <code class="highlighter-rouge">conv1</code>, followed by a max-pooling layer, <code class="highlighter-rouge">hpool1</code>, which reduces the size of the image. We then feed this image into another convolutional layer, <code class="highlighter-rouge">conv2</code>, followed by another max-pooling layer, <code class="highlighter-rouge">hpool2</code>, which reduces the image size further. We then have two consecutive fully connected layers, <code class="highlighter-rouge">fc1</code> and <code class="highlighter-rouge">fc2</code>, between which we use dropout (this randomly removes edges during each epoch of training to mitigate overfitting). Finally, we classify.</p>

<p>We’re going to use the ADAM adaptive moment optimizer, with a cross-entropy cost function.</p>

<h3 id="setup">Setup</h3>

<div class="highlighter-rouge"><pre class="highlight"><code>import tensorflow as tf
tf.set_random_seed(1234)


# convolution params
log_specgram_shape = df.iloc[0]["log_specgram"].shape
CONV_STRIDE_LENGTH = 1
CONV_WINDOW_LENGTH = 5
MAX_POOL_STRIDE_LENGTH = 2

# features
CONV_1_NUM_FEATURES = 32
CONV_2_NUM_FEATURES = 16
DENSE_NUM_FEATURES = 256

# training
NUM_LABELS = df.label.unique().size
BATCH_SIZE = 50
NUM_EPOCHS = 1000
LEARNING_RATE = 1e-4
LOG_TRAIN_STEPS = 1
</code></pre>
</div>

<h3 id="draw-the-computational-graph">Draw the computational graph</h3>

<div class="highlighter-rouge"><pre class="highlight"><code># This node is where we feed a batch of the training data and labels at each training step
x = tf.placeholder(tf.float32,shape=(None, *log_specgram_shape, 1))
y_ = tf.placeholder(tf.float32, shape=(None, len(df.label.unique())))


# Weight initialisation functions
 
# small noise for symmetry breaking and non-zero gradients
def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

# ReLU neurons - initialise with small positive bias to stop 'dead' neurons
def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)


def conv2d(x, W):  
    return tf.nn.conv2d(x, W, strides=[1, CONV_STRIDE_LENGTH, CONV_STRIDE_LENGTH, 1], padding='SAME')

# ksize is filter size
def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, MAX_POOL_STRIDE_LENGTH, MAX_POOL_STRIDE_LENGTH, 1],
                        strides=[1, MAX_POOL_STRIDE_LENGTH, MAX_POOL_STRIDE_LENGTH, 1], padding='SAME')
</code></pre>
</div>

<h4 id="first-convolutional-layer">First Convolutional Layer</h4>
<p>We can now implement our first layer. It will consist of convolution, followed by max pooling. The convolution will compute <code class="highlighter-rouge">CONV_1_NUM_FEATURES</code> features for each <code class="highlighter-rouge">CONV_WINDOW_LENGTH</code> $\times$ <code class="highlighter-rouge">CONV_WINDOW_LENGTH</code> patch. Its weight tensor will have a shape of <code class="highlighter-rouge">[CONV_WINDOW_LENGTH, CONV_WINDOW_LENGTH, 1, CONV_1_NUM_FEATURES]</code>. The first two dimensions are the patch size, the next is the number of input channels (mono audio, so <code class="highlighter-rouge">1</code>), and the last is the number of output channels. We will also have a bias vector with a component for each output channel.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>W_conv1 = weight_variable([CONV_WINDOW_LENGTH, CONV_WINDOW_LENGTH, 1, CONV_1_NUM_FEATURES])
b_conv1 = bias_variable([CONV_1_NUM_FEATURES])


h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
</code></pre>
</div>

<h4 id="second-convolutional-layer">Second Convolutional Layer</h4>

<div class="highlighter-rouge"><pre class="highlight"><code>W_conv2 = weight_variable([CONV_WINDOW_LENGTH, CONV_WINDOW_LENGTH, CONV_1_NUM_FEATURES, CONV_2_NUM_FEATURES])
b_conv2 = bias_variable([CONV_2_NUM_FEATURES])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)

# 2x2 maxpool gives image dimensions np.ceil(np.array(log_specgram_shape)/2).astype(int)
</code></pre>
</div>

<h4 id="densely-connected-layer">Densely Connected Layer</h4>

<p>Now that the image size has been reduced, we add a fully-connected layer with 256 neurons. We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, and apply a ReLU activation function.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>def scale_shape_maxpool2x2(shape_tuple):
    return np.ceil(np.array(shape_tuple)/2).astype(int)

log_specgram_shape_reduced = scale_shape_maxpool2x2(scale_shape_maxpool2x2(log_specgram_shape))

W_fc1 = weight_variable([np.prod(log_specgram_shape_reduced) * CONV_2_NUM_FEATURES, DENSE_NUM_FEATURES])
b_fc1 = bias_variable([DENSE_NUM_FEATURES])

h_pool2_flat = tf.reshape(h_pool2, [-1, np.prod(log_specgram_shape_reduced) * CONV_2_NUM_FEATURES])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
</code></pre>
</div>

<h4 id="dropout">Dropout</h4>

<p>To reduce overfitting, we will apply dropout before the readout layer. We create a <code class="highlighter-rouge">placeholder</code> for the probability that a neuron’s output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
</code></pre>
</div>

<h4 id="readout-layer">Readout Layer</h4>

<div class="highlighter-rouge"><pre class="highlight"><code>W_fc2 = weight_variable([DENSE_NUM_FEATURES, NUM_LABELS])
b_fc2 = bias_variable([NUM_LABELS])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
</code></pre>
</div>

<h3 id="training">Training</h3>

<h4 id="batching-function">Batching function</h4>
<p>We need a function to feed in batches of data for training.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>def return_batch(df, batch_size=10):
    batch_df = df.sample(batch_size)
    x = np.vstack(batch_df["log_specgram"]).reshape(batch_df.index.size, *log_specgram_shape, 1).astype(np.float32)
    y = np.vstack(batch_df["label_one_hot"]).astype(np.float32)
    return x, y
</code></pre>
</div>

<h4 id="time-logging">Time logging</h4>
<p>We want some rough idea of how long training is going to take. On my laptop it was around 14 hours! 😱</p>

<div class="highlighter-rouge"><pre class="highlight"><code>import time

def estimate_time_remaining(time_in, current_step, steps_gap, total_steps):
    current_time = time.time() - time_in
    time_per_step = current_time / steps_gap
    time_remaining = (total_steps - current_step) * time_per_step
    m, s = divmod(time_remaining, 60)
    h, m = divmod(m, 60)
    print("Approximately %d hours, %02d minutes, %02d seconds remaining." % (h, m, s))
</code></pre>
</div>

<h4 id="train-and-evaluate-the-model">Train and Evaluate the Model</h4>

<p>We’re using the numerically stable <code class="highlighter-rouge">tf.nn.softmax_cross_entropy_with_logits</code> function here. This is the long part.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

sess = tf.Session()
sess.run(tf.global_variables_initializer())

with sess.as_default():
    current_time = time.time()
    for i in range(NUM_EPOCHS):
        batch = return_batch(train_data, BATCH_SIZE)
        
        # logging
        if i % LOG_TRAIN_STEPS == 0:
            train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})
            print('Epoch %d, training accuracy %.3f' % (i, train_accuracy))
            estimate_time_remaining(current_time, i, LOG_TRAIN_STEPS, NUM_EPOCHS)
            current_time = time.time()

        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
</code></pre>
</div>

<p><em>Note</em>: I’ve deleted the output of the above cell to keep the script short.</p>

<h4 id="save-model-and-variables">Save model and variables</h4>

<div class="highlighter-rouge"><pre class="highlight"><code>with sess.as_default():
    saver = tf.train.Saver()
    save_path = saver.save(sess, "../data/model.ckpt")
    print("Model saved in file: %s" % save_path)

Model saved in file: ../data/model.ckpt
</code></pre>
</div>

<h3 id="testing">Testing</h3>

<p>Now we have a trained model, we want to test out how well it works on the test set.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>with sess.as_default():
    test_batch = return_batch(test_data, test_data.index.size)
    test_accuracy = accuracy.eval(feed_dict={x: test_batch[0], y_: test_batch[1], keep_prob: 1.0})
    print("Test accuracy: %.3f" % test_accuracy)

Test accuracy: 0.915
</code></pre>
</div>

<p>Yay! We have done a lot better than the baseline of 0.518.</p>

<div align="middle" style="padding-top:0.5cm"><img src="//assets/is_it_a_banger_files/great_success.gif" width="40%" /></div>

<h2 id="whats-next">What’s Next?</h2>

<p>It looks like our initial attempt with a ConvNet trained on log spectrogram data has worked well as a first attempt. However, there are a bunch of things we could think about to improve things:</p>

<ul>
  <li>
    <p><em>Feature selection</em>: the <code class="highlighter-rouge">librosa</code> library which generated the log spectrogram can compute a whole host of different audio features such as mel spectrogram and decompositions of the signal into percussive and melodic components.</p>
  </li>
  <li>
    <p><em>Inspecting misclassified data</em>: digging in to which audio clips were misclassified might give us insight into why they were misclassified. We could use this information to improve the model.</p>
  </li>
  <li>
    <p><em>ConvNet</em>: There are plenty of hyperparameters to tune here and even the architecture can be changed. Thinking more carefully about the structure of the input features and what design to use could help here.</p>
  </li>
  <li>
    <p><em>Other models</em>: perhaps another machine learning model, such as SVM or nearest neighbours classification could be more effective (and certainly would be quicker!)</p>
  </li>
</ul>

<p>Thanks for reading, and as Tommy H would say, keep it 💯.</p>

  </div>

</article>

        </main>
      </div>

    <footer class="t-hackcss-footer">
  <hr />

  <h3 class="footer-heading">Danial Dervovic</h3>

  <div class="grid t-hackcss-sm-reversed-grid">

    <div class="cell -5of12">
      <div class="contact-list">
        <p>
          Danial Dervovic,
          <a href="mailto:contact@danialdervovic.com">contact@danialdervovic.com</a>
        </p>

        
          
          <span class="t-hackcss-social">
            <i class="t-hackcss-icon"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path></svg>
</i>
            <a href="https://github.com/ddervs">ddervs</a>
          </span>
        
          
          <span class="t-hackcss-social">
            <i class="t-hackcss-icon"><svg x="0px" y="0px" viewBox="0 0 430.117 430.117" style="enable-background:new 0 0 430.117 430.117;" xml:space="preserve">
<g>
<path id="LinkedIn" d="M430.117,261.543V420.56h-92.188V272.193c0-37.271-13.334-62.707-46.703-62.707   c-25.473,0-40.632,17.142-47.301,33.724c-2.432,5.928-3.058,14.179-3.058,22.477V420.56h-92.219c0,0,1.242-251.285,0-277.32h92.21   v39.309c-0.187,0.294-0.43,0.611-0.606,0.896h0.606v-0.896c12.251-18.869,34.13-45.824,83.102-45.824   C384.633,136.724,430.117,176.361,430.117,261.543z M52.183,9.558C20.635,9.558,0,30.251,0,57.463   c0,26.619,20.038,47.94,50.959,47.94h0.616c32.159,0,52.159-21.317,52.159-47.94C103.128,30.251,83.734,9.558,52.183,9.558z    M5.477,420.56h92.184v-277.32H5.477V420.56z" fill="#828282"/>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g>
<g>
</g></svg>
</i>
            <a href="http://linkedin.com/in/danial-dervovic">danial-dervovic</a>
          </span>
        
      </div>
    </div>

    <div id="footer-spacer" class="cell -1of12"></div>

    <div class="cell -6of12">
      <p>Personal website, blog.
</p>
    </div>
  </div>

</footer>


    </div>

  </body>

</html>
